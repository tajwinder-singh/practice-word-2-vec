{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For installing nltk and spacy with correct numpy version\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade --force-reinstall numpy\n",
    "# !pip install --upgrade --force-reinstall scikit-learn nltk spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install gensim nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:31.450999Z",
     "iopub.status.busy": "2025-10-06T10:23:31.450832Z",
     "iopub.status.idle": "2025-10-06T10:23:34.101142Z",
     "shell.execute_reply": "2025-10-06T10:23:34.097967Z",
     "shell.execute_reply.started": "2025-10-06T10:23:31.450979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.104904Z",
     "iopub.status.busy": "2025-10-06T10:23:34.104542Z",
     "iopub.status.idle": "2025-10-06T10:23:34.146152Z",
     "shell.execute_reply": "2025-10-06T10:23:34.142110Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.104879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/sms-spam-classification-using-word2vec/spam.csv\", encoding = \"latin1\", header = 0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.148325Z",
     "iopub.status.busy": "2025-10-06T10:23:34.148133Z",
     "iopub.status.idle": "2025-10-06T10:23:34.156200Z",
     "shell.execute_reply": "2025-10-06T10:23:34.152312Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.148308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis =  1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.158029Z",
     "iopub.status.busy": "2025-10-06T10:23:34.157862Z",
     "iopub.status.idle": "2025-10-06T10:23:34.173735Z",
     "shell.execute_reply": "2025-10-06T10:23:34.168533Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.158014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.174437Z",
     "iopub.status.busy": "2025-10-06T10:23:34.174271Z",
     "iopub.status.idle": "2025-10-06T10:23:34.186772Z",
     "shell.execute_reply": "2025-10-06T10:23:34.181710Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.174422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.rename({'v1': 'labels', 'v2': 'message'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.188369Z",
     "iopub.status.busy": "2025-10-06T10:23:34.188198Z",
     "iopub.status.idle": "2025-10-06T10:23:34.202572Z",
     "shell.execute_reply": "2025-10-06T10:23:34.197184Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.188354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   labels   5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.203315Z",
     "iopub.status.busy": "2025-10-06T10:23:34.203141Z",
     "iopub.status.idle": "2025-10-06T10:23:34.214495Z",
     "shell.execute_reply": "2025-10-06T10:23:34.211589Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.203299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = np.where(df['labels'] == 'spam', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.217700Z",
     "iopub.status.busy": "2025-10-06T10:23:34.217514Z",
     "iopub.status.idle": "2025-10-06T10:23:34.231016Z",
     "shell.execute_reply": "2025-10-06T10:23:34.228184Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.217683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.234203Z",
     "iopub.status.busy": "2025-10-06T10:23:34.234037Z",
     "iopub.status.idle": "2025-10-06T10:23:34.649840Z",
     "shell.execute_reply": "2025-10-06T10:23:34.644344Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.234189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def smart_case(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    processed = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_type_ or token.pos_ in ['PROPN'] or token.text.isupper(): # Always use '_'.\n",
    "            processed.append(token.text)\n",
    "        else:\n",
    "            processed.append(token.text.lower())\n",
    "\n",
    "    return ' '.join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:23:34.651530Z",
     "iopub.status.busy": "2025-10-06T10:23:34.651351Z",
     "iopub.status.idle": "2025-10-06T10:24:19.182784Z",
     "shell.execute_reply": "2025-10-06T10:24:19.179608Z",
     "shell.execute_reply.started": "2025-10-06T10:23:34.651513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazy',\n",
       "  'available',\n",
       "  'bugis',\n",
       "  'n',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'e',\n",
       "  'buffet',\n",
       "  'Cine',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat'],\n",
       " ['ok', 'lar', 'joking', 'wif', 'u', 'oni'],\n",
       " ['free',\n",
       "  'entry',\n",
       "  '2',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'win',\n",
       "  'FA',\n",
       "  'Cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  '21st',\n",
       "  'May',\n",
       "  '2005',\n",
       "  'text',\n",
       "  'FA',\n",
       "  '87121',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question',\n",
       "  'std',\n",
       "  'txt',\n",
       "  'rate',\n",
       "  'T',\n",
       "  '&',\n",
       "  'C',\n",
       "  'apply',\n",
       "  '08452810075over18'],\n",
       " ['U', 'dun', 'say', 'early', 'hor', 'U', 'c', 'already', 'say'],\n",
       " ['nah', 'I', 'think', 'go', 'usf', 'life', 'around', 'though']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # Using this with Word2Vec is only applicabel if the dataset is small\n",
    "corpus = []\n",
    "final_corpus = []\n",
    "\n",
    "for i in range(len(df['message'])):\n",
    "    message = df['message'][i]\n",
    "    message = re.sub('[^a-zA-Z0-9\\s!$#%@=&]', ' ', message)\n",
    "    message = smart_case(message)\n",
    "    tokens = nltk.word_tokenize(message)\n",
    "    message = [lemmatizer.lemmatize(word) for word in tokens if word not in set(stopwords.words('english'))]\n",
    "    corpus.append(' '.join(message))\n",
    "\n",
    "\n",
    "final_corpus = [nltk.word_tokenize(doc) for doc in corpus] # Tokenizing again since we need a list of words for each sentence.\n",
    "\n",
    "final_corpus[:5]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:24:19.186152Z",
     "iopub.status.busy": "2025-10-06T10:24:19.185935Z",
     "iopub.status.idle": "2025-10-06T10:24:19.603911Z",
     "shell.execute_reply": "2025-10-06T10:24:19.599223Z",
     "shell.execute_reply.started": "2025-10-06T10:24:19.186123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(final_corpus, window = 5, vector_size = 100, min_count = 2, workers = 4) # use 50-100 for <10k, 100-200 for 10k-1M, and 200-300 for wikipedia, blogs.\n",
    "\n",
    "def get_average_vectors(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis = 0) if len(vectors) > 0 else np.zeros(model.vector_size) # If the length of the vectors is , 0 then it means the words in the sentence are not understood by the w2v_model or those words may be rare which may not be usefull(noise). That's why I am filling the vector with zeros in order to context it as 'no information' to the classifier_model.\n",
    "\n",
    "X = np.array([get_average_vectors(doc, w2v_model) for doc in final_corpus]) # This will take the average arrays of all the dimension of all words which will happen for each sentence.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T10:24:19.606443Z",
     "iopub.status.busy": "2025-10-06T10:24:19.606238Z",
     "iopub.status.idle": "2025-10-06T10:24:19.647923Z",
     "shell.execute_reply": "2025-10-06T10:24:19.643932Z",
     "shell.execute_reply.started": "2025-10-06T10:24:19.606424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  The metric, modifier and average arguments are used only for determining\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  The metric, modifier and average arguments are used only for determining\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  The metric, modifier and average arguments are used only for determining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       965\n",
      "           1       0.00      0.00      0.00       150\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.43      0.50      0.46      1115\n",
      "weighted avg       0.75      0.87      0.80      1115\n",
      "\n",
      "\n",
      "Accuracy: 0.8654708520179372\n",
      "Confusion Matrix:\n",
      "     [[965   0]\n",
      " [150   0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state =42, test_size = 0.2)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(f\"Classification Report:\\n   {classification_report(y_test, y_pred)}\\n\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(\"Confusion Matrix:\\n    \", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons Behind Bad Results:\n",
    "- ### We are getting bad results for both balanced and imbalanced data when compared to using TF-IDF(in previous project) because, the averaging of words in a sentence distorts the meaning of rare words.\n",
    "\n",
    "- ### Eg., \"Win a free iphone now.\" is a sentence and words like \"win\", \"free\" are rare/unique. When the model averages these words vector, it losts the meaning/context of rare words, causing lack of lack of contextual understanding in model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements:\n",
    "- ### W2V works well in large dataset."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8412947,
     "sourceId": 13275471,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
